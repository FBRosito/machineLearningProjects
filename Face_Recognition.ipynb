{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/FBRosito/machineLearningProjects/blob/main/Face_Recognition.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D7n109-gfKsJ",
        "outputId": "403945e0-9edf-4859-ae1d-98e16514baea",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "feAfj-5Gib9J",
        "outputId": "ea05846b-3a15-4d8a-e300-8702ec7e159b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "%cd /content/drive/My Drive/Colab Notebooks/Face_Recognition"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/drive/My Drive/Colab Notebooks/Face_Recognition\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a9oEs4j1ivxk"
      },
      "source": [
        "path='/content/drive/My Drive/Colab Notebooks/Face_Recognition'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8WTJ__FHi4mY"
      },
      "source": [
        "import os\n",
        "import glob"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JbYplJagkWc2"
      },
      "source": [
        "# Get Image names stored in \"Images\" folder\n",
        "image_path_names=[]\n",
        "person_names=set()\n",
        "for file_name in glob.glob(path+'/Images/*_[1-9]*.jpg'):\n",
        "  image_path_names.append(file_name)\n",
        "  person_names.add(image_path_names[-1].split('/')[-1].split('_')[0])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B0VQO0oRkoKe",
        "outputId": "a7cc06b0-96b2-4881-ed39-97062b798663",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "len(image_path_names)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "60"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BzJua7rHll-Q",
        "outputId": "e4ee005e-1d28-48e1-9749-d14773e229e9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "person_names"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'angelamerkel', 'jinping', 'lakshminarayana', 'modi', 'putin', 'trump'}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QrvHS5nkO0VX"
      },
      "source": [
        "  There are total 60 images containing 10 images per person."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MDEOeTvSn09H"
      },
      "source": [
        "# Download Dlib CNN face detector\n",
        "! wget http://dlib.net/files/mmod_human_face_detector.dat.bz2"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8ntLNPejsPU8"
      },
      "source": [
        "!bzip2 -dk mmod_human_face_detector.dat.bz2"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ugCOCE_lsTlP"
      },
      "source": [
        "%rm mmod_human_face_detector.dat.bz2"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4bRbWm4KlvMU"
      },
      "source": [
        "import cv2\n",
        "import matplotlib.pyplot as plt\n",
        "import dlib"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Vdo0aJNkA_sy"
      },
      "source": [
        "# Load CNN face detector into dlib\n",
        "dnnFaceDetector=dlib.cnn_face_detection_model_v1(\"mmod_human_face_detector.dat\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "21_IVMOgybPU"
      },
      "source": [
        "os.mkdir(path+'/Images_crop/')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UvnDaqCDIqmL"
      },
      "source": [
        "# For each person create a separate folder\n",
        "for person in person_names:\n",
        "  os.mkdir(path+'/Images_crop/'+person+'/')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MDUaECnUHp4r"
      },
      "source": [
        "# Detect face, crop detected face and save them in corresponding person folder\n",
        "for file_name in image_path_names:\n",
        "  img=cv2.imread(file_name)\n",
        "  gray=cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
        "  rects=dnnFaceDetector(gray,1)\n",
        "  left,top,right,bottom=0,0,0,0\n",
        "  for (i,rect) in enumerate(rects):\n",
        "    left=rect.rect.left() #x1\n",
        "    top=rect.rect.top() #y1\n",
        "    right=rect.rect.right() #x2\n",
        "    bottom=rect.rect.bottom() #y2\n",
        "  width=right-left\n",
        "  height=bottom-top\n",
        "  img_crop=img[top:top+height,left:left+width]\n",
        "  img_path=path+'/Images_crop/'+file_name.split('/')[-1].split('_')[0]+'/'+file_name.split('/')[-1]\n",
        "  cv2.imwrite(img_path,img_crop)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q0B5EsUzQIIZ"
      },
      "source": [
        "# Get Image names for testing\n",
        "test_image_path_names=[]\n",
        "for file_name in glob.glob(path+'/Images_test/*_[123].jpg'):\n",
        "  test_image_path_names.append(file_name)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KhLxtZuDRD-3",
        "outputId": "6d980d59-6bd3-4805-e149-6bfbe7ca95e3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "len(test_image_path_names)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "18"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 47
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q4g5jL7iRWdh"
      },
      "source": [
        "For each person 3 images to test in Images_test folder."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uXjqG2ZnRFTD"
      },
      "source": [
        "os.mkdir(path+'/Test_Images_crop/')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ff0ekPxwRwWG"
      },
      "source": [
        "# Create Separate folder for each person in \"Test_Images_crop\" folder\n",
        "for person in person_names:\n",
        "  os.mkdir(path+'/Test_Images_crop/'+person+'/')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-kjc0zokR12L"
      },
      "source": [
        "# Detect face,crop face and save in corresponding folder\n",
        "for file_name in test_image_path_names:\n",
        "  img=cv2.imread(file_name)\n",
        "  gray=cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
        "  rects=dnnFaceDetector(gray,1)\n",
        "  left,top,right,bottom=0,0,0,0\n",
        "  for (i,rect) in enumerate(rects):\n",
        "    left=rect.rect.left() #x1\n",
        "    top=rect.rect.top() #y1\n",
        "    right=rect.rect.right() #x2\n",
        "    bottom=rect.rect.bottom() #y2\n",
        "  width=right-left\n",
        "  height=bottom-top\n",
        "  img_crop=img[top:top+height,left:left+width]\n",
        "  img_path=path+'/Test_Images_crop/'+file_name.split('/')[-1].split('_')[0]+'/'+file_name.split('/')[-1]\n",
        "  cv2.imwrite(img_path,img_crop)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zSmSIDhoPNM7"
      },
      "source": [
        "<pre>Here images are sorted to corresponding test and train folders of same person\n",
        "Directory structure :\n",
        "|Images /\n",
        "|  |-- (60 images)\n",
        "|Images_crop /\n",
        "|  |--angelamerkel\n",
        "|     |--(10 images)\n",
        "|  |--jinping /\n",
        "|     |--(10 images)\n",
        "|  |--lakshminarayana /\n",
        "|         |--(10 imgaes)\n",
        "|  |--modi / (10 images)\n",
        "|  |--putin / (10 images)\n",
        "|  |--trump / (10 images)\n",
        "|Images_test /\n",
        "|  |-- .. / (18 images)\n",
        "|Images_test_crop /\n",
        "|  |--angelamerkel / (3 images)\n",
        "|  |--jinping / (3 images)\n",
        "|  |--lakshminarayana / (3 imgaes)\n",
        "|  |--modi / (3 images)\n",
        "|  |--putin / (3 images)\n",
        "|Face_Recognition.ipynb\n",
        "|mmod_human_face_detector.dat\n",
        "</pre>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K2vsqwHc4msl"
      },
      "source": [
        "! pip install gdown"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SofaX67Q-b0i",
        "outputId": "7a68f1ae-dd7a-42f8-e31d-b7ef815e0c57",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        }
      },
      "source": [
        "#Download pre-trained vgg-face-model-weights as .h5 file\n",
        "! gdown https://drive.google.com/uc?id=1CPSeum3HpopfomUEK1gybeuIVoeJT_Eo"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1CPSeum3HpopfomUEK1gybeuIVoeJT_Eo\n",
            "To: /content/drive/My Drive/Colab Notebooks/Face_Recognition/vgg_face_weights.h5\n",
            "580MB [00:06, 84.3MB/s]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Qbuvd_mX-lXD",
        "outputId": "23ae63c6-b662-4d33-95a6-f320ed235249",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "source": [
        "%ls"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Face_Recognition.ipynb  \u001b[0m\u001b[01;34mImages_test\u001b[0m/                  vgg_face_weights.h5\n",
            "\u001b[01;34mImages\u001b[0m/                 mmod_human_face_detector.dat\n",
            "\u001b[01;34mImages_crop\u001b[0m/            \u001b[01;34mTest_Images_crop\u001b[0m/\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qF0i2mrxBTR0"
      },
      "source": [
        "! pip install tensorflow==2.0.0"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Rp8xv8XO_JTS"
      },
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras.models import Sequential,Model\n",
        "from tensorflow.keras.layers import ZeroPadding2D,Convolution2D,MaxPooling2D\n",
        "from tensorflow.keras.layers import Dense,Dropout,Softmax,Flatten,Activation,BatchNormalization\n",
        "from tensorflow.keras.preprocessing.image import load_img,img_to_array\n",
        "from tensorflow.keras.applications.imagenet_utils import preprocess_input\n",
        "import tensorflow.keras.backend as K"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x4_j4qVk_Pze"
      },
      "source": [
        "#Define VGG_FACE_MODEL architecture\n",
        "model = Sequential()\n",
        "model.add(ZeroPadding2D((1,1),input_shape=(224,224, 3)))\n",
        "model.add(Convolution2D(64, (3, 3), activation='relu'))\n",
        "model.add(ZeroPadding2D((1,1)))\n",
        "model.add(Convolution2D(64, (3, 3), activation='relu'))\n",
        "model.add(MaxPooling2D((2,2), strides=(2,2)))\n",
        "model.add(ZeroPadding2D((1,1)))\n",
        "model.add(Convolution2D(128, (3, 3), activation='relu'))\n",
        "model.add(ZeroPadding2D((1,1)))\n",
        "model.add(Convolution2D(128, (3, 3), activation='relu'))\n",
        "model.add(MaxPooling2D((2,2), strides=(2,2)))\n",
        "model.add(ZeroPadding2D((1,1)))\n",
        "model.add(Convolution2D(256, (3, 3), activation='relu'))\n",
        "model.add(ZeroPadding2D((1,1)))\n",
        "model.add(Convolution2D(256, (3, 3), activation='relu'))\n",
        "model.add(ZeroPadding2D((1,1)))\n",
        "model.add(Convolution2D(256, (3, 3), activation='relu'))\n",
        "model.add(MaxPooling2D((2,2), strides=(2,2)))\n",
        "model.add(ZeroPadding2D((1,1)))\n",
        "model.add(Convolution2D(512, (3, 3), activation='relu'))\n",
        "model.add(ZeroPadding2D((1,1)))\n",
        "model.add(Convolution2D(512, (3, 3), activation='relu'))\n",
        "model.add(ZeroPadding2D((1,1)))\n",
        "model.add(Convolution2D(512, (3, 3), activation='relu'))\n",
        "model.add(MaxPooling2D((2,2), strides=(2,2)))\n",
        "model.add(ZeroPadding2D((1,1)))\n",
        "model.add(Convolution2D(512, (3, 3), activation='relu'))\n",
        "model.add(ZeroPadding2D((1,1)))\n",
        "model.add(Convolution2D(512, (3, 3), activation='relu'))\n",
        "model.add(ZeroPadding2D((1,1)))\n",
        "model.add(Convolution2D(512, (3, 3), activation='relu'))\n",
        "model.add(MaxPooling2D((2,2), strides=(2,2)))\n",
        "model.add(Convolution2D(4096, (7, 7), activation='relu'))\n",
        "model.add(Dropout(0.5))\n",
        "model.add(Convolution2D(4096, (1, 1), activation='relu'))\n",
        "model.add(Dropout(0.5))\n",
        "model.add(Convolution2D(2622, (1, 1)))\n",
        "model.add(Flatten())\n",
        "model.add(Activation('softmax'))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hQV4WAcvANH0"
      },
      "source": [
        "# Load VGG Face model weights\n",
        "model.load_weights('vgg_face_weights.h5')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u7k9g_wtCUfD",
        "outputId": "3f5b8306-f5ca-44fa-bf1b-ae32b79e4d89",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "model.summary()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "zero_padding2d (ZeroPadding2 (None, 226, 226, 3)       0         \n",
            "_________________________________________________________________\n",
            "conv2d (Conv2D)              (None, 224, 224, 64)      1792      \n",
            "_________________________________________________________________\n",
            "zero_padding2d_1 (ZeroPaddin (None, 226, 226, 64)      0         \n",
            "_________________________________________________________________\n",
            "conv2d_1 (Conv2D)            (None, 224, 224, 64)      36928     \n",
            "_________________________________________________________________\n",
            "max_pooling2d (MaxPooling2D) (None, 112, 112, 64)      0         \n",
            "_________________________________________________________________\n",
            "zero_padding2d_2 (ZeroPaddin (None, 114, 114, 64)      0         \n",
            "_________________________________________________________________\n",
            "conv2d_2 (Conv2D)            (None, 112, 112, 128)     73856     \n",
            "_________________________________________________________________\n",
            "zero_padding2d_3 (ZeroPaddin (None, 114, 114, 128)     0         \n",
            "_________________________________________________________________\n",
            "conv2d_3 (Conv2D)            (None, 112, 112, 128)     147584    \n",
            "_________________________________________________________________\n",
            "max_pooling2d_1 (MaxPooling2 (None, 56, 56, 128)       0         \n",
            "_________________________________________________________________\n",
            "zero_padding2d_4 (ZeroPaddin (None, 58, 58, 128)       0         \n",
            "_________________________________________________________________\n",
            "conv2d_4 (Conv2D)            (None, 56, 56, 256)       295168    \n",
            "_________________________________________________________________\n",
            "zero_padding2d_5 (ZeroPaddin (None, 58, 58, 256)       0         \n",
            "_________________________________________________________________\n",
            "conv2d_5 (Conv2D)            (None, 56, 56, 256)       590080    \n",
            "_________________________________________________________________\n",
            "zero_padding2d_6 (ZeroPaddin (None, 58, 58, 256)       0         \n",
            "_________________________________________________________________\n",
            "conv2d_6 (Conv2D)            (None, 56, 56, 256)       590080    \n",
            "_________________________________________________________________\n",
            "max_pooling2d_2 (MaxPooling2 (None, 28, 28, 256)       0         \n",
            "_________________________________________________________________\n",
            "zero_padding2d_7 (ZeroPaddin (None, 30, 30, 256)       0         \n",
            "_________________________________________________________________\n",
            "conv2d_7 (Conv2D)            (None, 28, 28, 512)       1180160   \n",
            "_________________________________________________________________\n",
            "zero_padding2d_8 (ZeroPaddin (None, 30, 30, 512)       0         \n",
            "_________________________________________________________________\n",
            "conv2d_8 (Conv2D)            (None, 28, 28, 512)       2359808   \n",
            "_________________________________________________________________\n",
            "zero_padding2d_9 (ZeroPaddin (None, 30, 30, 512)       0         \n",
            "_________________________________________________________________\n",
            "conv2d_9 (Conv2D)            (None, 28, 28, 512)       2359808   \n",
            "_________________________________________________________________\n",
            "max_pooling2d_3 (MaxPooling2 (None, 14, 14, 512)       0         \n",
            "_________________________________________________________________\n",
            "zero_padding2d_10 (ZeroPaddi (None, 16, 16, 512)       0         \n",
            "_________________________________________________________________\n",
            "conv2d_10 (Conv2D)           (None, 14, 14, 512)       2359808   \n",
            "_________________________________________________________________\n",
            "zero_padding2d_11 (ZeroPaddi (None, 16, 16, 512)       0         \n",
            "_________________________________________________________________\n",
            "conv2d_11 (Conv2D)           (None, 14, 14, 512)       2359808   \n",
            "_________________________________________________________________\n",
            "zero_padding2d_12 (ZeroPaddi (None, 16, 16, 512)       0         \n",
            "_________________________________________________________________\n",
            "conv2d_12 (Conv2D)           (None, 14, 14, 512)       2359808   \n",
            "_________________________________________________________________\n",
            "max_pooling2d_4 (MaxPooling2 (None, 7, 7, 512)         0         \n",
            "_________________________________________________________________\n",
            "conv2d_13 (Conv2D)           (None, 1, 1, 4096)        102764544 \n",
            "_________________________________________________________________\n",
            "dropout (Dropout)            (None, 1, 1, 4096)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_14 (Conv2D)           (None, 1, 1, 4096)        16781312  \n",
            "_________________________________________________________________\n",
            "dropout_1 (Dropout)          (None, 1, 1, 4096)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_15 (Conv2D)           (None, 1, 1, 2622)        10742334  \n",
            "_________________________________________________________________\n",
            "flatten (Flatten)            (None, 2622)              0         \n",
            "_________________________________________________________________\n",
            "activation (Activation)      (None, 2622)              0         \n",
            "=================================================================\n",
            "Total params: 145,002,878\n",
            "Trainable params: 145,002,878\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nZ21pD4FDLkT"
      },
      "source": [
        "# Remove Last Softmax layer and get model upto last flatten layer with outputs 2622 units\n",
        "vgg_face=Model(inputs=model.layers[0].input,outputs=model.layers[-2].output)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6zSqFno_Epfz"
      },
      "source": [
        "#Prepare Training Data\n",
        "x_train=[]\n",
        "y_train=[]\n",
        "person_folders=os.listdir(path+'/Images_crop/')\n",
        "person_rep=dict()\n",
        "for i,person in enumerate(person_folders):\n",
        "  person_rep[i]=person\n",
        "  image_names=os.listdir('Images_crop/'+person+'/')\n",
        "  for image_name in image_names:\n",
        "    img=load_img(path+'/Images_crop/'+person+'/'+image_name,target_size=(224,224))\n",
        "    img=img_to_array(img)\n",
        "    img=np.expand_dims(img,axis=0)\n",
        "    img=preprocess_input(img)\n",
        "    img_encode=vgg_face(img)\n",
        "    x_train.append(np.squeeze(K.eval(img_encode)).tolist())\n",
        "    y_train.append(i)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8Cl8ewCNRRCu",
        "outputId": "95a535f3-b2c9-4c05-ec7a-4633e28478e4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 119
        }
      },
      "source": [
        "person_rep"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{0: 'modi',\n",
              " 1: 'trump',\n",
              " 2: 'angelamerkel',\n",
              " 3: 'jinping',\n",
              " 4: 'lakshminarayana',\n",
              " 5: 'putin'}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 64
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QyxgU05lSX4D"
      },
      "source": [
        "x_train=np.array(x_train)\n",
        "y_train=np.array(y_train)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aylVlQO2SZXB"
      },
      "source": [
        "#Prepare Test Data\n",
        "x_test=[]\n",
        "y_test=[]\n",
        "person_folders=os.listdir(path+'/Test_Images_crop/')\n",
        "for i,person in enumerate(person_folders):\n",
        "  image_names=os.listdir('Test_Images_crop/'+person+'/')\n",
        "  for image_name in image_names:\n",
        "    img=load_img(path+'/Test_Images_crop/'+person+'/'+image_name,target_size=(224,224))\n",
        "    img=img_to_array(img)\n",
        "    img=np.expand_dims(img,axis=0)\n",
        "    img=preprocess_input(img)\n",
        "    img_encode=vgg_face(img)\n",
        "    x_test.append(np.squeeze(K.eval(img_encode)).tolist())\n",
        "    y_test.append(i)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C4NNw5lqTuKL"
      },
      "source": [
        "x_test=np.array(x_test)\n",
        "y_test=np.array(y_test)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EAT3ehPVT-24"
      },
      "source": [
        "# Save test and train data for later use\n",
        "np.save('train_data',x_train)\n",
        "np.save('train_labels',y_train)\n",
        "np.save('test_data',x_test)\n",
        "np.save('test_labels',y_test)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sQ5aWACqUnX3"
      },
      "source": [
        "# Load saved data\n",
        "x_train=np.load('train_data.npy')\n",
        "y_train=np.load('train_labels.npy')\n",
        "x_test=np.load('test_data.npy')\n",
        "y_test=np.load('test_labels.npy')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wZOEaNOb0qxD"
      },
      "source": [
        "# Softmax regressor to classify images based on encoding\n",
        "classifier_model=Sequential()\n",
        "classifier_model.add(Dense(units=100,input_dim=x_train.shape[1],kernel_initializer='glorot_uniform'))\n",
        "classifier_model.add(BatchNormalization())\n",
        "classifier_model.add(Activation('tanh'))\n",
        "classifier_model.add(Dropout(0.3))\n",
        "classifier_model.add(Dense(units=10,kernel_initializer='glorot_uniform'))\n",
        "classifier_model.add(BatchNormalization())\n",
        "classifier_model.add(Activation('tanh'))\n",
        "classifier_model.add(Dropout(0.2))\n",
        "classifier_model.add(Dense(units=6,kernel_initializer='he_uniform'))\n",
        "classifier_model.add(Activation('softmax'))\n",
        "classifier_model.compile(loss=tf.keras.losses.SparseCategoricalCrossentropy(),optimizer='nadam',metrics=['accuracy'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fNJsffbD4ulB",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "192feb14-2266-42b2-f26b-db35d637e456"
      },
      "source": [
        "classifier_model.fit(x_train,y_train,epochs=100,validation_data=(x_test,y_test))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 60 samples, validate on 18 samples\n",
            "Epoch 1/100\n",
            "60/60 [==============================] - 0s 697us/sample - loss: 0.1860 - accuracy: 1.0000 - val_loss: 0.1234 - val_accuracy: 1.0000\n",
            "Epoch 2/100\n",
            "60/60 [==============================] - 0s 557us/sample - loss: 0.2060 - accuracy: 1.0000 - val_loss: 0.1229 - val_accuracy: 1.0000\n",
            "Epoch 3/100\n",
            "60/60 [==============================] - 0s 526us/sample - loss: 0.1497 - accuracy: 1.0000 - val_loss: 0.1224 - val_accuracy: 1.0000\n",
            "Epoch 4/100\n",
            "60/60 [==============================] - 0s 590us/sample - loss: 0.1743 - accuracy: 1.0000 - val_loss: 0.1220 - val_accuracy: 1.0000\n",
            "Epoch 5/100\n",
            "60/60 [==============================] - 0s 516us/sample - loss: 0.1806 - accuracy: 1.0000 - val_loss: 0.1215 - val_accuracy: 1.0000\n",
            "Epoch 6/100\n",
            "60/60 [==============================] - 0s 516us/sample - loss: 0.1861 - accuracy: 1.0000 - val_loss: 0.1208 - val_accuracy: 1.0000\n",
            "Epoch 7/100\n",
            "60/60 [==============================] - 0s 567us/sample - loss: 0.1586 - accuracy: 1.0000 - val_loss: 0.1203 - val_accuracy: 1.0000\n",
            "Epoch 8/100\n",
            "60/60 [==============================] - 0s 511us/sample - loss: 0.1921 - accuracy: 1.0000 - val_loss: 0.1198 - val_accuracy: 1.0000\n",
            "Epoch 9/100\n",
            "60/60 [==============================] - 0s 514us/sample - loss: 0.1834 - accuracy: 0.9833 - val_loss: 0.1193 - val_accuracy: 1.0000\n",
            "Epoch 10/100\n",
            "60/60 [==============================] - 0s 730us/sample - loss: 0.1394 - accuracy: 0.9833 - val_loss: 0.1189 - val_accuracy: 1.0000\n",
            "Epoch 11/100\n",
            "60/60 [==============================] - 0s 547us/sample - loss: 0.1544 - accuracy: 1.0000 - val_loss: 0.1186 - val_accuracy: 1.0000\n",
            "Epoch 12/100\n",
            "60/60 [==============================] - 0s 611us/sample - loss: 0.1575 - accuracy: 1.0000 - val_loss: 0.1182 - val_accuracy: 1.0000\n",
            "Epoch 13/100\n",
            "60/60 [==============================] - 0s 562us/sample - loss: 0.2125 - accuracy: 1.0000 - val_loss: 0.1179 - val_accuracy: 1.0000\n",
            "Epoch 14/100\n",
            "60/60 [==============================] - 0s 522us/sample - loss: 0.1764 - accuracy: 1.0000 - val_loss: 0.1175 - val_accuracy: 1.0000\n",
            "Epoch 15/100\n",
            "60/60 [==============================] - 0s 526us/sample - loss: 0.2184 - accuracy: 1.0000 - val_loss: 0.1171 - val_accuracy: 1.0000\n",
            "Epoch 16/100\n",
            "60/60 [==============================] - 0s 574us/sample - loss: 0.1689 - accuracy: 1.0000 - val_loss: 0.1166 - val_accuracy: 1.0000\n",
            "Epoch 17/100\n",
            "60/60 [==============================] - 0s 554us/sample - loss: 0.1678 - accuracy: 1.0000 - val_loss: 0.1161 - val_accuracy: 1.0000\n",
            "Epoch 18/100\n",
            "60/60 [==============================] - 0s 510us/sample - loss: 0.1932 - accuracy: 1.0000 - val_loss: 0.1155 - val_accuracy: 1.0000\n",
            "Epoch 19/100\n",
            "60/60 [==============================] - 0s 554us/sample - loss: 0.1682 - accuracy: 0.9833 - val_loss: 0.1151 - val_accuracy: 1.0000\n",
            "Epoch 20/100\n",
            "60/60 [==============================] - 0s 565us/sample - loss: 0.1684 - accuracy: 1.0000 - val_loss: 0.1148 - val_accuracy: 1.0000\n",
            "Epoch 21/100\n",
            "60/60 [==============================] - 0s 567us/sample - loss: 0.1691 - accuracy: 1.0000 - val_loss: 0.1146 - val_accuracy: 1.0000\n",
            "Epoch 22/100\n",
            "60/60 [==============================] - 0s 575us/sample - loss: 0.2233 - accuracy: 1.0000 - val_loss: 0.1141 - val_accuracy: 1.0000\n",
            "Epoch 23/100\n",
            "60/60 [==============================] - 0s 590us/sample - loss: 0.1632 - accuracy: 1.0000 - val_loss: 0.1136 - val_accuracy: 1.0000\n",
            "Epoch 24/100\n",
            "60/60 [==============================] - 0s 558us/sample - loss: 0.1646 - accuracy: 0.9833 - val_loss: 0.1132 - val_accuracy: 1.0000\n",
            "Epoch 25/100\n",
            "60/60 [==============================] - 0s 626us/sample - loss: 0.1761 - accuracy: 1.0000 - val_loss: 0.1129 - val_accuracy: 1.0000\n",
            "Epoch 26/100\n",
            "60/60 [==============================] - 0s 676us/sample - loss: 0.1952 - accuracy: 1.0000 - val_loss: 0.1125 - val_accuracy: 1.0000\n",
            "Epoch 27/100\n",
            "60/60 [==============================] - 0s 600us/sample - loss: 0.1620 - accuracy: 1.0000 - val_loss: 0.1121 - val_accuracy: 1.0000\n",
            "Epoch 28/100\n",
            "60/60 [==============================] - 0s 633us/sample - loss: 0.1660 - accuracy: 1.0000 - val_loss: 0.1117 - val_accuracy: 1.0000\n",
            "Epoch 29/100\n",
            "60/60 [==============================] - 0s 615us/sample - loss: 0.1912 - accuracy: 1.0000 - val_loss: 0.1110 - val_accuracy: 1.0000\n",
            "Epoch 30/100\n",
            "60/60 [==============================] - 0s 564us/sample - loss: 0.1751 - accuracy: 0.9833 - val_loss: 0.1105 - val_accuracy: 1.0000\n",
            "Epoch 31/100\n",
            "60/60 [==============================] - 0s 628us/sample - loss: 0.1615 - accuracy: 1.0000 - val_loss: 0.1100 - val_accuracy: 1.0000\n",
            "Epoch 32/100\n",
            "60/60 [==============================] - 0s 523us/sample - loss: 0.1429 - accuracy: 0.9833 - val_loss: 0.1097 - val_accuracy: 1.0000\n",
            "Epoch 33/100\n",
            "60/60 [==============================] - 0s 517us/sample - loss: 0.1471 - accuracy: 0.9833 - val_loss: 0.1095 - val_accuracy: 1.0000\n",
            "Epoch 34/100\n",
            "60/60 [==============================] - 0s 565us/sample - loss: 0.1679 - accuracy: 1.0000 - val_loss: 0.1091 - val_accuracy: 1.0000\n",
            "Epoch 35/100\n",
            "60/60 [==============================] - 0s 497us/sample - loss: 0.1494 - accuracy: 1.0000 - val_loss: 0.1087 - val_accuracy: 1.0000\n",
            "Epoch 36/100\n",
            "60/60 [==============================] - 0s 589us/sample - loss: 0.1727 - accuracy: 0.9833 - val_loss: 0.1083 - val_accuracy: 1.0000\n",
            "Epoch 37/100\n",
            "60/60 [==============================] - 0s 629us/sample - loss: 0.1370 - accuracy: 1.0000 - val_loss: 0.1078 - val_accuracy: 1.0000\n",
            "Epoch 38/100\n",
            "60/60 [==============================] - 0s 752us/sample - loss: 0.1977 - accuracy: 1.0000 - val_loss: 0.1072 - val_accuracy: 1.0000\n",
            "Epoch 39/100\n",
            "60/60 [==============================] - 0s 624us/sample - loss: 0.1572 - accuracy: 1.0000 - val_loss: 0.1069 - val_accuracy: 1.0000\n",
            "Epoch 40/100\n",
            "60/60 [==============================] - 0s 577us/sample - loss: 0.1464 - accuracy: 1.0000 - val_loss: 0.1064 - val_accuracy: 1.0000\n",
            "Epoch 41/100\n",
            "60/60 [==============================] - 0s 628us/sample - loss: 0.1561 - accuracy: 1.0000 - val_loss: 0.1061 - val_accuracy: 1.0000\n",
            "Epoch 42/100\n",
            "60/60 [==============================] - 0s 595us/sample - loss: 0.1974 - accuracy: 0.9833 - val_loss: 0.1057 - val_accuracy: 1.0000\n",
            "Epoch 43/100\n",
            "60/60 [==============================] - 0s 549us/sample - loss: 0.2378 - accuracy: 0.9833 - val_loss: 0.1052 - val_accuracy: 1.0000\n",
            "Epoch 44/100\n",
            "60/60 [==============================] - 0s 590us/sample - loss: 0.1666 - accuracy: 1.0000 - val_loss: 0.1047 - val_accuracy: 1.0000\n",
            "Epoch 45/100\n",
            "60/60 [==============================] - 0s 641us/sample - loss: 0.1802 - accuracy: 1.0000 - val_loss: 0.1044 - val_accuracy: 1.0000\n",
            "Epoch 46/100\n",
            "60/60 [==============================] - 0s 622us/sample - loss: 0.1770 - accuracy: 1.0000 - val_loss: 0.1041 - val_accuracy: 1.0000\n",
            "Epoch 47/100\n",
            "60/60 [==============================] - 0s 594us/sample - loss: 0.1839 - accuracy: 0.9833 - val_loss: 0.1038 - val_accuracy: 1.0000\n",
            "Epoch 48/100\n",
            "60/60 [==============================] - 0s 575us/sample - loss: 0.1727 - accuracy: 1.0000 - val_loss: 0.1035 - val_accuracy: 1.0000\n",
            "Epoch 49/100\n",
            "60/60 [==============================] - 0s 557us/sample - loss: 0.1906 - accuracy: 1.0000 - val_loss: 0.1032 - val_accuracy: 1.0000\n",
            "Epoch 50/100\n",
            "60/60 [==============================] - 0s 618us/sample - loss: 0.1315 - accuracy: 1.0000 - val_loss: 0.1029 - val_accuracy: 1.0000\n",
            "Epoch 51/100\n",
            "60/60 [==============================] - 0s 563us/sample - loss: 0.1436 - accuracy: 1.0000 - val_loss: 0.1027 - val_accuracy: 1.0000\n",
            "Epoch 52/100\n",
            "60/60 [==============================] - 0s 542us/sample - loss: 0.1867 - accuracy: 1.0000 - val_loss: 0.1023 - val_accuracy: 1.0000\n",
            "Epoch 53/100\n",
            "60/60 [==============================] - 0s 538us/sample - loss: 0.1427 - accuracy: 1.0000 - val_loss: 0.1021 - val_accuracy: 1.0000\n",
            "Epoch 54/100\n",
            "60/60 [==============================] - 0s 574us/sample - loss: 0.1803 - accuracy: 1.0000 - val_loss: 0.1018 - val_accuracy: 1.0000\n",
            "Epoch 55/100\n",
            "60/60 [==============================] - 0s 552us/sample - loss: 0.1608 - accuracy: 1.0000 - val_loss: 0.1017 - val_accuracy: 1.0000\n",
            "Epoch 56/100\n",
            "60/60 [==============================] - 0s 609us/sample - loss: 0.1676 - accuracy: 1.0000 - val_loss: 0.1013 - val_accuracy: 1.0000\n",
            "Epoch 57/100\n",
            "60/60 [==============================] - 0s 562us/sample - loss: 0.1571 - accuracy: 1.0000 - val_loss: 0.1011 - val_accuracy: 1.0000\n",
            "Epoch 58/100\n",
            "60/60 [==============================] - 0s 612us/sample - loss: 0.1496 - accuracy: 0.9833 - val_loss: 0.1006 - val_accuracy: 1.0000\n",
            "Epoch 59/100\n",
            "60/60 [==============================] - 0s 543us/sample - loss: 0.1854 - accuracy: 1.0000 - val_loss: 0.1003 - val_accuracy: 1.0000\n",
            "Epoch 60/100\n",
            "60/60 [==============================] - 0s 510us/sample - loss: 0.1846 - accuracy: 1.0000 - val_loss: 0.0999 - val_accuracy: 1.0000\n",
            "Epoch 61/100\n",
            "60/60 [==============================] - 0s 585us/sample - loss: 0.1671 - accuracy: 1.0000 - val_loss: 0.0994 - val_accuracy: 1.0000\n",
            "Epoch 62/100\n",
            "60/60 [==============================] - 0s 538us/sample - loss: 0.1409 - accuracy: 1.0000 - val_loss: 0.0991 - val_accuracy: 1.0000\n",
            "Epoch 63/100\n",
            "60/60 [==============================] - 0s 579us/sample - loss: 0.1609 - accuracy: 0.9833 - val_loss: 0.0986 - val_accuracy: 1.0000\n",
            "Epoch 64/100\n",
            "60/60 [==============================] - 0s 571us/sample - loss: 0.1576 - accuracy: 0.9833 - val_loss: 0.0983 - val_accuracy: 1.0000\n",
            "Epoch 65/100\n",
            "60/60 [==============================] - 0s 770us/sample - loss: 0.1534 - accuracy: 1.0000 - val_loss: 0.0980 - val_accuracy: 1.0000\n",
            "Epoch 66/100\n",
            "60/60 [==============================] - 0s 626us/sample - loss: 0.1633 - accuracy: 1.0000 - val_loss: 0.0977 - val_accuracy: 1.0000\n",
            "Epoch 67/100\n",
            "60/60 [==============================] - 0s 581us/sample - loss: 0.1792 - accuracy: 1.0000 - val_loss: 0.0974 - val_accuracy: 1.0000\n",
            "Epoch 68/100\n",
            "60/60 [==============================] - 0s 593us/sample - loss: 0.1293 - accuracy: 1.0000 - val_loss: 0.0971 - val_accuracy: 1.0000\n",
            "Epoch 69/100\n",
            "60/60 [==============================] - 0s 595us/sample - loss: 0.1380 - accuracy: 1.0000 - val_loss: 0.0970 - val_accuracy: 1.0000\n",
            "Epoch 70/100\n",
            "60/60 [==============================] - 0s 695us/sample - loss: 0.1529 - accuracy: 1.0000 - val_loss: 0.0968 - val_accuracy: 1.0000\n",
            "Epoch 71/100\n",
            "60/60 [==============================] - 0s 650us/sample - loss: 0.1612 - accuracy: 1.0000 - val_loss: 0.0965 - val_accuracy: 1.0000\n",
            "Epoch 72/100\n",
            "60/60 [==============================] - 0s 570us/sample - loss: 0.1441 - accuracy: 1.0000 - val_loss: 0.0962 - val_accuracy: 1.0000\n",
            "Epoch 73/100\n",
            "60/60 [==============================] - 0s 621us/sample - loss: 0.1494 - accuracy: 1.0000 - val_loss: 0.0960 - val_accuracy: 1.0000\n",
            "Epoch 74/100\n",
            "60/60 [==============================] - 0s 618us/sample - loss: 0.1619 - accuracy: 1.0000 - val_loss: 0.0957 - val_accuracy: 1.0000\n",
            "Epoch 75/100\n",
            "60/60 [==============================] - 0s 606us/sample - loss: 0.1737 - accuracy: 0.9833 - val_loss: 0.0952 - val_accuracy: 1.0000\n",
            "Epoch 76/100\n",
            "60/60 [==============================] - 0s 590us/sample - loss: 0.1242 - accuracy: 1.0000 - val_loss: 0.0950 - val_accuracy: 1.0000\n",
            "Epoch 77/100\n",
            "60/60 [==============================] - 0s 634us/sample - loss: 0.1274 - accuracy: 1.0000 - val_loss: 0.0948 - val_accuracy: 1.0000\n",
            "Epoch 78/100\n",
            "60/60 [==============================] - 0s 612us/sample - loss: 0.1478 - accuracy: 1.0000 - val_loss: 0.0944 - val_accuracy: 1.0000\n",
            "Epoch 79/100\n",
            "60/60 [==============================] - 0s 654us/sample - loss: 0.1398 - accuracy: 1.0000 - val_loss: 0.0941 - val_accuracy: 1.0000\n",
            "Epoch 80/100\n",
            "60/60 [==============================] - 0s 650us/sample - loss: 0.1874 - accuracy: 1.0000 - val_loss: 0.0938 - val_accuracy: 1.0000\n",
            "Epoch 81/100\n",
            "60/60 [==============================] - 0s 580us/sample - loss: 0.1714 - accuracy: 0.9833 - val_loss: 0.0936 - val_accuracy: 1.0000\n",
            "Epoch 82/100\n",
            "60/60 [==============================] - 0s 601us/sample - loss: 0.1657 - accuracy: 1.0000 - val_loss: 0.0933 - val_accuracy: 1.0000\n",
            "Epoch 83/100\n",
            "60/60 [==============================] - 0s 639us/sample - loss: 0.1864 - accuracy: 1.0000 - val_loss: 0.0931 - val_accuracy: 1.0000\n",
            "Epoch 84/100\n",
            "60/60 [==============================] - 0s 684us/sample - loss: 0.1498 - accuracy: 1.0000 - val_loss: 0.0929 - val_accuracy: 1.0000\n",
            "Epoch 85/100\n",
            "60/60 [==============================] - 0s 573us/sample - loss: 0.1665 - accuracy: 1.0000 - val_loss: 0.0927 - val_accuracy: 1.0000\n",
            "Epoch 86/100\n",
            "60/60 [==============================] - 0s 632us/sample - loss: 0.1175 - accuracy: 1.0000 - val_loss: 0.0925 - val_accuracy: 1.0000\n",
            "Epoch 87/100\n",
            "60/60 [==============================] - 0s 635us/sample - loss: 0.1426 - accuracy: 1.0000 - val_loss: 0.0921 - val_accuracy: 1.0000\n",
            "Epoch 88/100\n",
            "60/60 [==============================] - 0s 605us/sample - loss: 0.1188 - accuracy: 1.0000 - val_loss: 0.0919 - val_accuracy: 1.0000\n",
            "Epoch 89/100\n",
            "60/60 [==============================] - 0s 581us/sample - loss: 0.1223 - accuracy: 1.0000 - val_loss: 0.0915 - val_accuracy: 1.0000\n",
            "Epoch 90/100\n",
            "60/60 [==============================] - 0s 559us/sample - loss: 0.1426 - accuracy: 1.0000 - val_loss: 0.0913 - val_accuracy: 1.0000\n",
            "Epoch 91/100\n",
            "60/60 [==============================] - 0s 758us/sample - loss: 0.1104 - accuracy: 1.0000 - val_loss: 0.0911 - val_accuracy: 1.0000\n",
            "Epoch 92/100\n",
            "60/60 [==============================] - 0s 488us/sample - loss: 0.1631 - accuracy: 1.0000 - val_loss: 0.0909 - val_accuracy: 1.0000\n",
            "Epoch 93/100\n",
            "60/60 [==============================] - 0s 577us/sample - loss: 0.1850 - accuracy: 0.9500 - val_loss: 0.0906 - val_accuracy: 1.0000\n",
            "Epoch 94/100\n",
            "60/60 [==============================] - 0s 601us/sample - loss: 0.1586 - accuracy: 1.0000 - val_loss: 0.0904 - val_accuracy: 1.0000\n",
            "Epoch 95/100\n",
            "60/60 [==============================] - 0s 606us/sample - loss: 0.1726 - accuracy: 1.0000 - val_loss: 0.0902 - val_accuracy: 1.0000\n",
            "Epoch 96/100\n",
            "60/60 [==============================] - 0s 543us/sample - loss: 0.1702 - accuracy: 1.0000 - val_loss: 0.0900 - val_accuracy: 1.0000\n",
            "Epoch 97/100\n",
            "60/60 [==============================] - 0s 539us/sample - loss: 0.1418 - accuracy: 1.0000 - val_loss: 0.0897 - val_accuracy: 1.0000\n",
            "Epoch 98/100\n",
            "60/60 [==============================] - 0s 529us/sample - loss: 0.1549 - accuracy: 1.0000 - val_loss: 0.0894 - val_accuracy: 1.0000\n",
            "Epoch 99/100\n",
            "60/60 [==============================] - 0s 580us/sample - loss: 0.1434 - accuracy: 1.0000 - val_loss: 0.0892 - val_accuracy: 1.0000\n",
            "Epoch 100/100\n",
            "60/60 [==============================] - 0s 617us/sample - loss: 0.1362 - accuracy: 1.0000 - val_loss: 0.0888 - val_accuracy: 1.0000\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7f06ab03f668>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 59
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5BtgAfSo8XW1"
      },
      "source": [
        "# Save model for later use\n",
        "tf.keras.models.save_model(classifier_model,'/content/drive/My Drive/Colab Notebooks/Face_Recognition/face_classifier_model.h5')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3c-QvTzD_Xmf"
      },
      "source": [
        "# Load saved model\n",
        "classifier_model=tf.keras.models.load_model('/content/drive/My Drive/Colab Notebooks/Face_Recognition/face_classifier_model.h5')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I6_DyNnVAHuM"
      },
      "source": [
        "# Path to folder which contains images to be tested and predicted\n",
        "test_images_path=path+'/Test_Images/'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3aYRHru9AtfM"
      },
      "source": [
        "dnnFaceDetector=dlib.cnn_face_detection_model_v1(\"mmod_human_face_detector.dat\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AJZg7e93Cgu1"
      },
      "source": [
        "def plot(img):\n",
        "  plt.figure(figsize=(8,4))\n",
        "  plt.imshow(img[:,:,::-1])\n",
        "  plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wofuBLTOH7Iu"
      },
      "source": [
        "# Label names for class numbers\n",
        "person_rep={0:'Narendra Modi',1:'Donald Trump',2:'Angela Merkel',3:'Xi Jinping',4:'Lakshmi Narayana',5:'Vladimir Putin'}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0M57Q7chO_Xc"
      },
      "source": [
        "os.mkdir(path+'/Predictions')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oJPEhDUbBHUH"
      },
      "source": [
        "for img_name in os.listdir('Test_Images/'):\n",
        "  if img_name=='crop_img.jpg':\n",
        "    continue\n",
        "  # Load Image\n",
        "  img=cv2.imread(path+'/Test_Images/'+img_name)\n",
        "  gray=cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
        "  # Detect Faces\n",
        "  rects=dnnFaceDetector(gray,1)\n",
        "  left,top,right,bottom=0,0,0,0\n",
        "  for (i,rect) in enumerate(rects):\n",
        "    # Extract Each Face\n",
        "    left=rect.rect.left() #x1\n",
        "    top=rect.rect.top() #y1\n",
        "    right=rect.rect.right() #x2\n",
        "    bottom=rect.rect.bottom() #y2\n",
        "    width=right-left\n",
        "    height=bottom-top\n",
        "    img_crop=img[top:top+height,left:left+width]\n",
        "    cv2.imwrite(path+'/Test_Images/crop_img.jpg',img_crop)\n",
        "\n",
        "    # Get Embeddings\n",
        "    crop_img=load_img(path+'/Test_Images/crop_img.jpg',target_size=(224,224))\n",
        "    crop_img=img_to_array(crop_img)\n",
        "    crop_img=np.expand_dims(crop_img,axis=0)\n",
        "    crop_img=preprocess_input(crop_img)\n",
        "    img_encode=vgg_face(crop_img)\n",
        "\n",
        "    # Make Predictions\n",
        "    embed=K.eval(img_encode)\n",
        "    person=classifier_model.predict(embed)\n",
        "    name=person_rep[np.argmax(person)]\n",
        "    os.remove(path+'/Test_Images/crop_img.jpg')\n",
        "    cv2.rectangle(img,(left,top),(right,bottom),(0,255,0), 2)\n",
        "    img=cv2.putText(img,name,(left,top-10),cv2.FONT_HERSHEY_SIMPLEX,1,(255,0,255),2,cv2.LINE_AA)\n",
        "    img=cv2.putText(img,str(np.max(person)),(right,bottom+10),cv2.FONT_HERSHEY_SIMPLEX,0.5,(0,0,255),1,cv2.LINE_AA)\n",
        "  # Save images with bounding box,name and accuracy\n",
        "  cv2.imwrite(path+'/Predictions/'+img_name,img)\n",
        "  plot(img)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}